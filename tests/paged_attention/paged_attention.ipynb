{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from typing import Optional\n",
    "from extension_builder.builders.paged_attention import PagedAttentionBuilder\n",
    "import random\n",
    "from vllm.utils import get_max_shared_memory_bytes\n",
    "\n",
    "import utils as test_utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOAT32_BYTES = torch.finfo(torch.float).bits // 8\n",
    "# This will change depending on the compute capability.\n",
    "# - 512 as a buffer\n",
    "MAX_SEQ_LEN = 2 ** 10 #get_max_shared_memory_bytes() // FLOAT32_BYTES - 512\n",
    "NUM_BLOCKS = 40 #40000  # Arbitrary values for testing\n",
    "PARTITION_SIZE = 512\n",
    "\n",
    "DTYPES = [torch.half, torch.bfloat16, torch.float]\n",
    "NUM_GEN_SEQS = [1]  # Arbitrary values for testing\n",
    "NUM_PREFILL_SEQS = [3]  # Arbitrary values for testing\n",
    "NUM_HEADS = [(4,4)]#[(40, 40), (64, 8)]  # Arbitrary values for testing\n",
    "HEAD_SIZES = [32, 64, 80, 96, 112, 128, 256]\n",
    "BLOCK_SIZES = [16, 32]\n",
    "USE_ALIBI = [False, True]\n",
    "SEEDS = [0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seqs = NUM_GEN_SEQS[0]\n",
    "num_heads = NUM_HEADS[0]\n",
    "head_size = HEAD_SIZES[0]\n",
    "use_alibi = USE_ALIBI[0]\n",
    "block_size = BLOCK_SIZES[0]\n",
    "dtype = DTYPES[0]\n",
    "seed = SEEDS[0]\n",
    "MAX_SEQ_LEN = MAX_SEQ_LEN\n",
    "NUM_BLOCKS = NUM_BLOCKS\n",
    "device = \"cuda\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, keys, values, ref_out = test_utils.run_test(num_seqs, num_heads, head_size, \n",
    "                                    use_alibi, block_size, dtype, \n",
    "                                    seed, MAX_SEQ_LEN, NUM_BLOCKS, \n",
    "                                    device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeromeku/Cpp/CUDA/extension_builder/src/extension_builder/builders/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
      "  warnings.warn(\n",
      "The input conditions for extension module paged_attention have changed. Bumping to version 2 and re-building as paged_attention_v2...\n",
      "Emitting ninja build file /home/jeromeku/.cache/extension_builder/torch_extensions/torch2.1_cu12.1/build.ninja...\n",
      "Building extension module paged_attention_v2...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] c++ -MMD -MF attention_extension.o.d -DTORCH_EXTENSION_NAME=paged_attention_v2 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/jeromeku/Cpp/CUDA/extension_builder/csrc/ops/paged_attention -I/usr/local/cuda/include -isystem /home/jeromeku/miniconda3/envs/cutlass/lib/python3.10/site-packages/torch/include -isystem /home/jeromeku/miniconda3/envs/cutlass/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/jeromeku/miniconda3/envs/cutlass/lib/python3.10/site-packages/torch/include/TH -isystem /home/jeromeku/miniconda3/envs/cutlass/lib/python3.10/site-packages/torch/include/THC -isystem /home/jeromeku/miniconda3/envs/cutlass/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -c /home/jeromeku/Cpp/CUDA/extension_builder/csrc/ops/paged_attention/attention_extension.cpp -o attention_extension.o \n",
      "[2/3] c++ -MMD -MF attention.o.d -DTORCH_EXTENSION_NAME=paged_attention_v2 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/jeromeku/Cpp/CUDA/extension_builder/csrc/ops/paged_attention -I/usr/local/cuda/include -isystem /home/jeromeku/miniconda3/envs/cutlass/lib/python3.10/site-packages/torch/include -isystem /home/jeromeku/miniconda3/envs/cutlass/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/jeromeku/miniconda3/envs/cutlass/lib/python3.10/site-packages/torch/include/TH -isystem /home/jeromeku/miniconda3/envs/cutlass/lib/python3.10/site-packages/torch/include/THC -isystem /home/jeromeku/miniconda3/envs/cutlass/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -c /home/jeromeku/Cpp/CUDA/extension_builder/csrc/ops/paged_attention/attention.cpp -o attention.o \n",
      "[3/3] c++ attention.o attention_extension.o -shared -L/home/jeromeku/miniconda3/envs/cutlass/lib/python3.10/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o paged_attention_v2.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module paged_attention_v2...\n"
     ]
    }
   ],
   "source": [
    "lib = PagedAttentionBuilder().load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = test_utils.prepare_attention_inputs(\n",
    "        num_seqs, num_heads, head_size, use_alibi, block_size, dtype, seed, \n",
    "        MAX_SEQ_LEN, NUM_BLOCKS, device=device\n",
    "        )\n",
    "\n",
    "output_ext = torch.empty_like(args.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "res = lib.ref_single_query_cached_kv_attention(output_ext, \n",
    "                                               args.query, args.num_queries_per_kv, \n",
    "                                               args.key_cache, args.value_cache, \n",
    "                                               args.block_tables, args.context_lens,\n",
    "                                               args.scale, args.alibi_slopes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_q, ext_k, ext_v, ext_out = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(ext_out[0], ref_out[0], atol=1e-4, rtol=1e-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cutlass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
